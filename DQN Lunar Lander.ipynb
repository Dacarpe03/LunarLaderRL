{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TT3LPGc5PuK"
   },
   "source": [
    "\n",
    "\n",
    "<p><img height=\"80px\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/Escudo/EscUpm.jpg\" align=\"left\" hspace=\"0px\" vspace=\"0px\"></p>\n",
    "\n",
    "**Course \"Artificial Neural Networks and Deep Learning\" - Universidad Polit√©cnica de Madrid (UPM)**\n",
    "\n",
    "# **Deep Q-Learning for Cartpole**\n",
    "\n",
    "This notebook includes an implementation of the Deep Q-learning (DQN) algorithm for the cartpole problem (see [OpenAI's Cartpole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXBzOdaLAEUn"
   },
   "source": [
    "##Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2248,
     "status": "ok",
     "timestamp": 1669155674457,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "LjLS1WetFhCE"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrRuhN1AQ-s"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669155674459,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4N2yVwtuFlBu"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 100000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EXPLORATION_MAX = 1\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "NUMBER_OF_EPISODES = 400\n",
    "MAX_STEPS = 1000\n",
    "USE_TARGET = True\n",
    "LAST_N_SCORES = 30\n",
    "STEPS_TO_UPDATE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoGaas6TAd6p"
   },
   "source": [
    "## Class ReplayMemory\n",
    "\n",
    "Memory of transitions for experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669155674460,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "cQV7IfhFOoSh"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self,number_of_observations):\n",
    "        # Create replay memory\n",
    "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
    "        self.rewards = np.zeros(MEMORY_SIZE)\n",
    "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
    "        self.current_size=0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
    "        # Store a transition (s,a,r,s') in the replay memory\n",
    "        i = self.current_size\n",
    "        self.states[i] = state\n",
    "        self.states_next[i] = state_next\n",
    "        self.actions[i] = action\n",
    "        self.rewards[i] = reward\n",
    "        self.terminal_states[i] = terminal_state\n",
    "        self.current_size = i + 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        # Generate a sample of transitions from the replay memory\n",
    "        batch = np.random.choice(self.current_size, batch_size)\n",
    "        states = self.states[batch]\n",
    "        states_next = self.states_next[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        actions = self.actions[batch]   \n",
    "        terminal_states = self.terminal_states[batch]  \n",
    "        return states, actions, rewards, states_next, terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gejKO0OYAsS4"
   },
   "source": [
    "## Class DQN\n",
    "\n",
    "Reinforcement learning agent with a Deep Q-Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1669155674462,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "NZ6P4Gj0FtnU"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, number_of_observations, number_of_actions, use_target=True):\n",
    "        # Initialize variables and create neural model\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.use_target = use_target\n",
    "        self.number_of_actions = number_of_actions\n",
    "        self.number_of_observations = number_of_observations\n",
    "        self.scores = []\n",
    "        self.memory = ReplayMemory(number_of_observations)\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, terminal_state):\n",
    "        # Store a tuple (s, a, r, s') for experience replay\n",
    "        state = np.reshape(state, [1, self.number_of_observations])\n",
    "        next_state = np.reshape(next_state, [1, self.number_of_observations])\n",
    "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
    "\n",
    "    def select(self, state):\n",
    "        # Generate an action for a given state using epsilon-greedy policy\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.number_of_actions)\n",
    "        else:\n",
    "            state = np.reshape(state, [1, self.number_of_observations])\n",
    "            q_values = self.model.predict(state, verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def learn(self):\n",
    "        # Learn the value Q using a sample of examples from the replay memory\n",
    "        if self.memory.current_size < BATCH_SIZE: return\n",
    "\n",
    "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
    "        \n",
    "        if self.use_target:\n",
    "            q_targets = self.target_model.predict(states, verbose=0)\n",
    "            q_next_states = self.target_model.predict(next_states, verbose=0)\n",
    "        else:\n",
    "            q_targets = self.model.predict(states, verbose=0)\n",
    "            q_next_states = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "             if (terminal_states[i]):\n",
    "                  q_targets[i][actions[i]] = rewards[i]\n",
    "             else:\n",
    "                  q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])    \n",
    "\n",
    "        self.model.train_on_batch(states, q_targets)\n",
    "\n",
    "        # Decrease exploration rate\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def add_score(self, score):\n",
    "       # Add the obtained score in a list to be presented later\n",
    "        self.scores.append(score)\n",
    "\n",
    "    def display_scores_graphically(self, file_name=None):\n",
    "        # Display the obtained scores graphically\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        if file_name is not None:\n",
    "            plt.savefig(file_name)\n",
    "    \n",
    "    def create_model(self):\n",
    "        # Creates the models\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(24, input_shape=(number_of_observations,), \\\n",
    "                             activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.Dense(24, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.Dense(number_of_actions, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        if (self.use_target):\n",
    "            print(\"Updating target model weights\")\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def save_model(self, model_name):\n",
    "        self.model.save(model_name)\n",
    "    \n",
    "    def get_mean_last_n_scores(self, n):\n",
    "        scores_list = self.scores[-n:]\n",
    "        np_scores = np.array(scores_list)\n",
    "        scores_mean = np.mean(np_scores, axis=0)\n",
    "        return scores_mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-YSpziT0K9I"
   },
   "source": [
    "## Environment Lunar Lander\n",
    "\n",
    "Lunar Lander simulator from Open Ai Gym:\n",
    "\n",
    "<p><img height=\"200px\" src=\"https://raw.githubusercontent.com/martin-molina/reinforcement_learning/main/images/cartpole_attributes.png\" align=\"center\" vspace=\"20px\"</p>\n",
    "\n",
    "State vector:\n",
    "- state[0]: position\n",
    "- state[1]: velocity\n",
    "- state[2]: angle\n",
    "- state[3]: angular velocity\n",
    "\n",
    "Actions:\n",
    "- 0 (push cart to the left)\n",
    "- 1 (push cart to the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1669155674464,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4LBloUSG0LmT"
   },
   "outputs": [],
   "source": [
    "def create_environment(render=False):\n",
    "    # Create simulated environment\n",
    "    if render:\n",
    "        environment = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "    else:\n",
    "        environment = gym.make(\"LunarLander-v2\")\n",
    "    number_of_observations = environment.observation_space.shape[0]\n",
    "    number_of_actions = environment.action_space.n\n",
    "    return environment, number_of_observations, number_of_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbw6blhDcsJ"
   },
   "source": [
    "## Main program\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add episode score, the episode score will be equal to the sum of rewards in each step of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 805850,
     "status": "ok",
     "timestamp": 1669156480300,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "yuzI0m5u5vVf",
    "outputId": "92ec257e-d53d-4e34-c02f-c58a6938ad9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Episode   1: score -120.24766752691582 (exploration rate: 0.82, transitions: 71 time  7 )\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Episode   2: score -215.48195786667702 (exploration rate: 0.60, transitions: 132 time  9 )\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n",
      "Updating target model weights\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m state \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(end_episode):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Select an action for the current state\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Execute the action in the environment\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     state_next, reward, terminal_state, truncated, info \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn [4], line 27\u001b[0m, in \u001b[0;36mDQN.select\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_observations])\n\u001b[1;32m---> 27\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "environment, number_of_observations, number_of_actions = create_environment(render=True)\n",
    "agent = DQN(number_of_observations, number_of_actions, use_target=USE_TARGET)\n",
    "episode = 0\n",
    "goal_reached = False\n",
    "start_time = time.perf_counter()\n",
    "update_countdown = STEPS_TO_UPDATE\n",
    "\n",
    "while (episode < NUMBER_OF_EPISODES) and not (goal_reached):\n",
    "    start_episode_time = time.perf_counter()\n",
    "    episode += 1\n",
    "    episode_score = 0\n",
    "    step = 1\n",
    "    end_episode = False\n",
    "    state = environment.reset()[0]\n",
    "    \n",
    "    while not(end_episode):\n",
    "        # Select an action for the current state\n",
    "        action = agent.select(state)\n",
    "        # Execute the action in the environment\n",
    "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
    "        \n",
    "        # Update score:\n",
    "        episode_score += reward\n",
    "        # Store in memory the transition (s,a,r,s') \n",
    "        agent.remember(state, action, reward, state_next, terminal_state)\n",
    "\n",
    "        # Learn using a batch of experience stored in memory\n",
    "        agent.learn()\n",
    "  \n",
    "        # Detect end of episode and print\n",
    "        if terminal_state or step >= MAX_STEPS:\n",
    "            agent.add_score(episode_score)\n",
    "            scores_mean = agent.get_mean_last_n_scores(LAST_N_SCORES)\n",
    "            if scores_mean >= 200: goal_reached = True\n",
    "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
    "            print(\"score {0:>3} \".format(episode_score), end = '') \n",
    "            print(\"(exploration rate: %.2f, \" % agent.exploration_rate, end = '')\n",
    "            print(\"transitions:\", str(agent.memory.current_size), end = ', ')\n",
    "            print(\"time \", round(time.perf_counter() - start_episode_time), \")\" )\n",
    "            end_episode = True \n",
    "        else:\n",
    "            state = state_next\n",
    "            step += 1\n",
    "            update_countdown -=1\n",
    "            \n",
    "        if update_countdown <= 0:\n",
    "            agent.update_target_model()\n",
    "            update_countdown = STEPS_TO_UPDATE\n",
    "        \n",
    "if goal_reached: print(\"Reached goal sucessfully.\")\n",
    "else: print(\"Failure to reach the goal.\")\n",
    "\n",
    "total_time = round((time.perf_counter() - start_time)/60)\n",
    "print (\"Time:\", total_time, \"minutes\")\n",
    "\n",
    "environment.close()\n",
    "agent.display_scores_graphically()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_name = 'model'\n",
    "COLUMNS = [\n",
    "    'model_name',\n",
    "    'gamma',\n",
    "    'memory_size',\n",
    "    'learning_rate',\n",
    "    'batch_size',\n",
    "    'exploration_max',\n",
    "    'exploration_min',\n",
    "    'exploration_decay',\n",
    "    'number_of_episodes',\n",
    "    'max_steps',\n",
    "    'use_target',\n",
    "    'training_time',\n",
    "    'used_episodes',\n",
    "    'goal_reached',\n",
    "    'last_n_scores',\n",
    "    'steps_to_update',\n",
    "    'scores_mean'\n",
    "]\n",
    "\n",
    "experiment_result = [[\n",
    "    model_name,\n",
    "    GAMMA,\n",
    "    MEMORY_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    BATCH_SIZE,\n",
    "    EXPLORATION_MAX,\n",
    "    EXPLORATION_MIN,\n",
    "    EXPLORATION_DECAY,\n",
    "    NUMBER_OF_EPISODES,\n",
    "    MAX_STEPS,\n",
    "    USE_TARGET,\n",
    "    total_time,\n",
    "    episode,\n",
    "    goal_reached,\n",
    "    LAST_N_SCORES,\n",
    "    STEPS_TO_UPDATE,\n",
    "    agent.get_mean_last_n_scores(LAST_N_SCORES)\n",
    "]]\n",
    "\n",
    "new_experiment_df = pd.DataFrame(experiment_result, columns=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = 'experiments_history.csv'\n",
    "experiments_history_df = pd.read_csv(history_path, index_col=0)\n",
    "concatenation = pd.concat([experiments_history_df, new_experiment_df], ignore_index=True)\n",
    "concatenation.to_csv(history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = f\"{model_name}.hdf5\"\n",
    "agent.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph_name = f\"{model_name}_training.png\"\n",
    "agent.display_scores_graphically(file_name=model_graph_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
