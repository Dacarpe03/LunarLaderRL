{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TT3LPGc5PuK"
   },
   "source": [
    "\n",
    "\n",
    "<p><img height=\"80px\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/UPM/Escudo/EscUpm.jpg\" align=\"left\" hspace=\"0px\" vspace=\"0px\"></p>\n",
    "\n",
    "**Course \"Artificial Neural Networks and Deep Learning\" - Universidad Polit√©cnica de Madrid (UPM)**\n",
    "\n",
    "# **Deep Q-Learning for Cartpole**\n",
    "\n",
    "This notebook includes an implementation of the Deep Q-learning (DQN) algorithm for the cartpole problem (see [OpenAI's Cartpole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXBzOdaLAEUn"
   },
   "source": [
    "##Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2248,
     "status": "ok",
     "timestamp": 1669155674457,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "LjLS1WetFhCE"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrRuhN1AQ-s"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669155674459,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4N2yVwtuFlBu"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 100000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EXPLORATION_MAX = 1\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "NUMBER_OF_EPISODES = 300\n",
    "MAX_STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoGaas6TAd6p"
   },
   "source": [
    "## Class ReplayMemory\n",
    "\n",
    "Memory of transitions for experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1669155674460,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "cQV7IfhFOoSh"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self,number_of_observations):\n",
    "        # Create replay memory\n",
    "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
    "        self.rewards = np.zeros(MEMORY_SIZE)\n",
    "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
    "        self.current_size=0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
    "        # Store a transition (s,a,r,s') in the replay memory\n",
    "        i = self.current_size\n",
    "        self.states[i] = state\n",
    "        self.states_next[i] = state_next\n",
    "        self.actions[i] = action\n",
    "        self.rewards[i] = reward\n",
    "        self.terminal_states[i] = terminal_state\n",
    "        self.current_size = i + 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        # Generate a sample of transitions from the replay memory\n",
    "        batch = np.random.choice(self.current_size, batch_size)\n",
    "        states = self.states[batch]\n",
    "        states_next = self.states_next[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        actions = self.actions[batch]   \n",
    "        terminal_states = self.terminal_states[batch]  \n",
    "        return states, actions, rewards, states_next, terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gejKO0OYAsS4"
   },
   "source": [
    "## Class DQN\n",
    "\n",
    "Reinforcement learning agent with a Deep Q-Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1669155674462,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "NZ6P4Gj0FtnU"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, number_of_observations, number_of_actions, target_model=True):\n",
    "        # Initialize variables and create neural model\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.number_of_actions = number_of_actions\n",
    "        self.number_of_observations = number_of_observations\n",
    "        self.scores = []\n",
    "        self.memory = ReplayMemory(number_of_observations)\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, terminal_state):\n",
    "        # Store a tuple (s, a, r, s') for experience replay\n",
    "        state = np.reshape(state, [1, self.number_of_observations])\n",
    "        next_state = np.reshape(next_state, [1, self.number_of_observations])\n",
    "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
    "\n",
    "    def select(self, state):\n",
    "        # Generate an action for a given state using epsilon-greedy policy\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.number_of_actions)\n",
    "        else:\n",
    "            state = np.reshape(state, [1, self.number_of_observations])\n",
    "            q_values = self.model.predict(state, verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def learn(self):\n",
    "        # Learn the value Q using a sample of examples from the replay memory\n",
    "        if self.memory.current_size < BATCH_SIZE: return\n",
    "\n",
    "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
    "\n",
    "        q_targets = self.target_model.predict(states, verbose=0)\n",
    "        q_next_states = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "             if (terminal_states[i]):\n",
    "                  q_targets[i][actions[i]] = rewards[i]\n",
    "             else:\n",
    "                  q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])    \n",
    "\n",
    "        self.model.train_on_batch(states, q_targets)\n",
    "\n",
    "        # Decrease exploration rate\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def add_score(self, score):\n",
    "       # Add the obtained score in a list to be presented later\n",
    "        self.scores.append(score)\n",
    "\n",
    "    def display_scores_graphically(self):\n",
    "        # Display the obtained scores graphically\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\\\n",
    "    \n",
    "    def create_model(self):\n",
    "        # Creates the models\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(24, input_shape=(number_of_observations,), \\\n",
    "                             activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.Dense(24, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.Dense(number_of_actions, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        print(\"Updating target model weights\")\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.model.save('model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-YSpziT0K9I"
   },
   "source": [
    "## Environment Lunar Lander\n",
    "\n",
    "Lunar Lander simulator from Open Ai Gym:\n",
    "\n",
    "<p><img height=\"200px\" src=\"https://raw.githubusercontent.com/martin-molina/reinforcement_learning/main/images/cartpole_attributes.png\" align=\"center\" vspace=\"20px\"</p>\n",
    "\n",
    "State vector:\n",
    "- state[0]: position\n",
    "- state[1]: velocity\n",
    "- state[2]: angle\n",
    "- state[3]: angular velocity\n",
    "\n",
    "Actions:\n",
    "- 0 (push cart to the left)\n",
    "- 1 (push cart to the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1669155674464,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4LBloUSG0LmT"
   },
   "outputs": [],
   "source": [
    "def create_environment(render=False):\n",
    "    # Create simulated environment\n",
    "    if render:\n",
    "        environment = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "    else:\n",
    "        environment = gym.make(\"LunarLander-v2\")\n",
    "    number_of_observations = environment.observation_space.shape[0]\n",
    "    number_of_actions = environment.action_space.n\n",
    "    return environment, number_of_observations, number_of_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbw6blhDcsJ"
   },
   "source": [
    "## Main program\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add episode score, the episode score will be equal to the sum of rewards in each step of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 805850,
     "status": "ok",
     "timestamp": 1669156480300,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "yuzI0m5u5vVf",
    "outputId": "92ec257e-d53d-4e34-c02f-c58a6938ad9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating target model weights\n",
      "Episode   1: score -262.0404557856934 (exploration rate: 0.68, transitions: 107 time  13 )\n",
      "Updating target model weights\n",
      "Episode   2: score -146.14045765281676 (exploration rate: 0.48, transitions: 177 time  9 )\n",
      "Updating target model weights\n",
      "Episode   3: score -172.58574563078588 (exploration rate: 0.28, transitions: 284 time  15 )\n",
      "Updating target model weights\n",
      "Episode   4: score -329.08480704841384 (exploration rate: 0.10, transitions: 496 time  31 )\n",
      "Updating target model weights\n",
      "Episode   5: score -116.03927141902139 (exploration rate: 0.06, transitions: 582 time  14 )\n",
      "Updating target model weights\n",
      "Episode   6: score -121.94457922602619 (exploration rate: 0.05, transitions: 645 time  10 )\n",
      "Updating target model weights\n",
      "Episode   7: score -117.00996930401926 (exploration rate: 0.01, transitions: 1645 time  156 )\n",
      "Updating target model weights\n",
      "Episode   8: score -177.3099097691215 (exploration rate: 0.01, transitions: 2406 time  118 )\n",
      "Updating target model weights\n",
      "Episode   9: score -117.64077168765124 (exploration rate: 0.01, transitions: 3406 time  161 )\n",
      "Updating target model weights\n",
      "Episode  10: score -237.2332264615446 (exploration rate: 0.01, transitions: 3776 time  58 )\n",
      "Updating target model weights\n",
      "Episode  11: score -168.21658511166873 (exploration rate: 0.01, transitions: 4051 time  43 )\n",
      "Updating target model weights\n",
      "Episode  12: score -253.34831793028937 (exploration rate: 0.01, transitions: 4467 time  65 )\n",
      "Updating target model weights\n"
     ]
    }
   ],
   "source": [
    "environment, number_of_observations, number_of_actions = create_environment(render=True)\n",
    "agent = DQN(number_of_observations, number_of_actions)\n",
    "episode = 0\n",
    "goal_reached = False\n",
    "start_time = time.perf_counter()\n",
    "while (episode < NUMBER_OF_EPISODES) and not (goal_reached):\n",
    "    start_episode_time = time.perf_counter()\n",
    "    episode += 1\n",
    "    episode_score = 0\n",
    "    step = 1\n",
    "    end_episode = False\n",
    "    state = environment.reset()[0]\n",
    "    \n",
    "    while not(end_episode):\n",
    "        # Select an action for the current state\n",
    "        action = agent.select(state)\n",
    "        # Execute the action in the environment\n",
    "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
    "        \n",
    "        # Update score:\n",
    "        episode_score += reward\n",
    "        # Store in memory the transition (s,a,r,s') \n",
    "        agent.remember(state, action, reward, state_next, terminal_state)\n",
    "\n",
    "        # Learn using a batch of experience stored in memory\n",
    "        agent.learn()\n",
    "  \n",
    "        # Detect end of episode and print\n",
    "        if terminal_state or step >= MAX_STEPS:\n",
    "            agent.add_score(episode_score)\n",
    "            if episode_score >= 200: goal_reached = True\n",
    "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
    "            print(\"score {0:>3} \".format(episode_score), end = '') \n",
    "            print(\"(exploration rate: %.2f, \" % agent.exploration_rate, end = '')\n",
    "            print(\"transitions:\", str(agent.memory.current_size), end = ' ')\n",
    "            print(\"time \", round(time.perf_counter() - start_episode_time), \")\" )\n",
    "            end_episode = True \n",
    "        else:\n",
    "            state = state_next\n",
    "            step += 1\n",
    "            \n",
    "    agent.update_target_model()\n",
    "        \n",
    "if goal_reached: print(\"Reached goal sucessfully.\")\n",
    "else: print(\"Failure to reach the goal.\")\n",
    "\n",
    "print (\"Time:\", round((time.perf_counter() - start_time)/60), \"minutes\")\n",
    "\n",
    "environment.close()\n",
    "agent.display_scores_graphically()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
